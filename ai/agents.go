package ai

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	"github.com/google/generative-ai-go/genai"
	"github.com/zaynkorai/mailflow/prompts"
	"google.golang.org/api/option"
)

type LLMClient interface {
	GenerateContent(ctx context.Context, parts ...genai.Part) (*genai.GenerateContentResponse, error)
}

type GeminiClient struct {
	model *genai.GenerativeModel
}

func NewGeminiClient(ctx context.Context, apiKey string) (*GeminiClient, error) {
	client, err := genai.NewClient(ctx, option.WithAPIKey(apiKey))
	if err != nil {
		return nil, fmt.Errorf("error creating Gemini client: %w", err)
	}

	model := client.GenerativeModel("gemini-2.0-flash")
	model.SetTemperature(0.1)

	return &GeminiClient{model: model}, nil
}

func (g *GeminiClient) GenerateContent(ctx context.Context, parts ...genai.Part) (*genai.GenerateContentResponse, error) {
	resp, err := g.model.GenerateContent(ctx, parts...)
	if err != nil {
		return nil, fmt.Errorf("failed to generate content with Gemini: %w", err)
	}
	return resp, nil
}

// CallLLMWithStructuredOutput is a generic function to call an LLM and parse its JSON output.
// T is the type of the struct you expect as output (e.g., CategorizeEmailOutput, ConstructRagQueriesOutput)
func CallLLMWithStructuredOutput[T any](ctx context.Context, client *GeminiClient, prompt string) (*T, error) {

	resp, err := client.model.GenerateContent(ctx, genai.Text(prompt))
	if err != nil {
		return nil, fmt.Errorf("failed to generate content from LLM: %w", err)
	}

	if len(resp.Candidates) == 0 || len(resp.Candidates[0].Content.Parts) == 0 {
		return nil, fmt.Errorf("LLM returned no candidates or no content")
	}

	rawLLMResponse := fmt.Sprint(resp.Candidates[0].Content.Parts[0])

	cleanedResponse := strings.TrimSpace(rawLLMResponse)
	cleanedResponse = strings.TrimPrefix(cleanedResponse, "```json")
	cleanedResponse = strings.TrimSuffix(cleanedResponse, "```")
	cleanedResponse = strings.TrimSpace(cleanedResponse)

	var output T
	err = json.Unmarshal([]byte(cleanedResponse), &output)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal LLM response to struct: %w. Cleaned Response: %s", err, cleanedResponse)
	}

	return &output, nil
}

func CallLLMForTextOutput(ctx context.Context, client LLMClient, prompt string) (string, error) {
	resp, err := client.GenerateContent(ctx, genai.Text(prompt))
	if err != nil {
		return "", fmt.Errorf("LLM call failed: %w", err)
	}

	if resp == nil || len(resp.Candidates) == 0 || len(resp.Candidates[0].Content.Parts) == 0 {
		return "", fmt.Errorf("no content generated by LLM")
	}

	var sb strings.Builder
	for _, part := range resp.Candidates[0].Content.Parts {
		if text, ok := part.(genai.Text); ok {
			sb.WriteString(string(text))
		}
	}
	return sb.String(), nil
}

type Agents struct {
	gemini *GeminiClient
}

func NewAgents(ctx context.Context, googleAPIKey string) (*Agents, error) {
	geminiClient, err := NewGeminiClient(ctx, googleAPIKey)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize Gemini client: %w", err)
	}

	return &Agents{
		gemini: geminiClient,
	}, nil
}

func (a *Agents) CategorizeEmail(ctx context.Context, emailBody string) (*CategorizeEmailOutput, error) {
	prompt := fmt.Sprintf(prompts.CATEGORIZE_EMAIL, emailBody)
	output, err := CallLLMWithStructuredOutput[CategorizeEmailOutput](ctx, a.gemini, prompt)
	if err != nil {
		return nil, fmt.Errorf("failed to categorize email: %w", err)
	}
	return output, nil
}

func (a *Agents) DesignRAGQueries(ctx context.Context, emailBody string) (*RAGQueriesOutput, error) {
	prompt := fmt.Sprintf(prompts.GENERATE_RAG_QUERIES, emailBody)
	output, err := CallLLMWithStructuredOutput[RAGQueriesOutput](ctx, a.gemini, prompt)
	if err != nil {
		return nil, fmt.Errorf("failed to design RAG queries: %w", err)
	}
	return output, nil
}

func (a *Agents) GenerateRAGAnswer(ctx context.Context, contextStr, question string) (string, error) {
	prompt := fmt.Sprintf(prompts.GENERATE_RAG_ANSWER, contextStr, question)
	answer, err := CallLLMForTextOutput(ctx, a.gemini, prompt)
	if err != nil {
		return "", fmt.Errorf("failed to generate RAG answer: %w", err)
	}
	return answer, nil
}

func (a *Agents) EmailWriter(ctx context.Context, emailInformation string, history []string) (*WriterOutput, error) {
	fullPrompt := prompts.EMAIL_WRITER + "\n\n"
	if len(history) > 0 {
		fullPrompt += "History of previous drafts and feedback:\n" + strings.Join(history, "\n") + "\n\n"
	}
	fullPrompt += "Instructions:\n" + emailInformation

	output, err := CallLLMWithStructuredOutput[WriterOutput](ctx, a.gemini, fullPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to write email draft: %w", err)
	}
	return output, nil
}

func (a *Agents) EmailProofreader(ctx context.Context, initialEmail, generatedEmail string) (*ProofReaderOutput, error) {
	prompt := fmt.Sprintf(prompts.EMAIL_PROOFREADER, initialEmail, generatedEmail)
	output, err := CallLLMWithStructuredOutput[ProofReaderOutput](ctx, a.gemini, prompt)
	if err != nil {
		return nil, fmt.Errorf("failed to proofread email: %w", err)
	}
	return output, nil
}

type EmailCategory string

const (
	ProductEnquiry    EmailCategory = "PRODUCT_ENQUIRY"
	CustomerComplaint EmailCategory = "CUSTOMER_COMPLAINT"
	CustomerFeedback  EmailCategory = "CUSTOMER_FEEDBACK"
	Unrelated         EmailCategory = "UNRELATED"
)

// CategorizeEmailOutput models the expected output from the email categorization agent.
type CategorizeEmailOutput struct {
	Category EmailCategory `json:"category"`
}

// RAGQueriesOutput models the expected output for RAG queries.
type RAGQueriesOutput struct {
	Queries []string `json:"queries"`
}

type WriterOutput struct {
	Email string `json:"email_content"`
}

// ProofReaderOutput models the expected output from the email proofreader agent.
type ProofReaderOutput struct {
	Feedback string `json:"feedback"`
	Send     bool   `json:"send"`
}
